# -*- coding: utf-8 -*-
"""Perth Housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_zd29xwXsEDYMJpWMXE8YgmuHJFdIGEu

**Reading and Understanding the Data**
"""

#importing libaries
import pandas as pd
import numpy as np

#data visualization libaries
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from urllib.request import urlopen
import json

#model building
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler

all_data = pd.read_csv('/content/drive/MyDrive/Perth Housing.csv')
all_data.head()

#display attribute information
all_data.info()

print(all_data.describe())

num_rows, num_columns = all_data.shape
print("Number of Rows:", num_rows)
print("Number of Columns:", num_columns)

all_data.columns

all_data.isnull().sum()

#replace float dtype to int dtype in GARAGE and BUILD_YEAR
cols = ['GARAGE', 'BUILD_YEAR']
all_data[cols] = all_data[cols].applymap(np.int64)

#remove '\r' from DATE_SOLD values and formate it to datetime type
all_data['DATE_SOLD'] = all_data['DATE_SOLD'].str.replace('\r', '')
all_data['DATE_SOLD'] = pd.to_datetime(all_data['DATE_SOLD'])

all_data.dtypes

"""**Finding strong correlations**"""

num_cols = list(all_data.select_dtypes(['int64', 'float64']))
print('Numerical Cols: ', num_cols,'\n')

cat_cols = list(all_data.select_dtypes(['object', 'datetime64[ns]']))
print('Object Cols:', cat_cols)

corr_matrix = all_data[num_cols].corr().round(1)

print(corr_matrix['PRICE'].sort_values(ascending=False))

heat_map_fig = px.imshow(corr_matrix, template="seaborn", text_auto=True)

heat_map_fig.update_layout(xaxis_rangeslider_visible=False,
                      autosize=False,
                      width=1150,
                      height=700,
                      title={
                            'text': "Correlation of Numerical Variables"}
                    )

heat_map_fig.show()

"""Conclusions:

*   Have corr with PRICE(>0.2): BATHROOMS, BEDROOMS, FLOOR_AREA
*   Have small corr(>=0.1): GARAGE
*   No corr(<=0): NEAREST_SCH_DIST, NEAREST_STN_DIST, BUILD_YEAR, POSTCODE,
     LONGITUDE, CBD_DIST, NEAREST_SCH_RANK, LATITUDE,LAND,AREA
     

"""

plt.figure(figsize=(8, 6))
sns.histplot(all_data["PRICE"], kde=True)
plt.title("Distribution of PRICE")
plt.show()

# Extract relevant columns
build_year = all_data['BUILD_YEAR']
prices = all_data['PRICE']

# Calculate average prices for each build year
average_prices_by_year = all_data.groupby('BUILD_YEAR')['PRICE'].mean()

# Create a line plot for average prices by build year
plt.figure(figsize=(10, 6))
plt.plot(average_prices_by_year.index, average_prices_by_year.values, marker='', color='blue')
plt.xlabel('Build Year')
plt.ylabel('Average Housing Price')
plt.title('Average Housing Prices by Build Year')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualizing Geographical Data


with urlopen('https://raw.githubusercontent.com/tonywr71/GeoJson-Data/master/australian-states.json') as response:
    australia = json.load(response)

info = geo_df = all_data[['LATITUDE', 'LONGITUDE', 'PRICE', 'ADDRESS', 'POSTCODE']]

fig = px.scatter_mapbox(info, lat='LATITUDE', lon='LONGITUDE', hover_name='ADDRESS',
    hover_data=['POSTCODE'], color='PRICE',
    zoom=8, height=500, width=1150)

fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":1,"l":1,"b":1})
fig.show()

# Summary:

# Homes with prices above $1.5 million are situated close to the ocean and in the center of Perth

# Houses under $0.5 million are located closer to the right side of town or, in general, outside of Perth center

# Price is affected by the population density

# Extract relevant columns
nearest_school_dist = all_data['NEAREST_SCH_DIST']
nearest_station_dist = all_data['NEAREST_STN_DIST']
cbd_dist = all_data['CBD_DIST']
prices = all_data['PRICE']

# Create separate scatter plots for each feature
plt.figure(figsize=(13, 6))

# Scatter plot for Nearest School Distance
plt.subplot(1, 3, 1)
plt.scatter(nearest_school_dist, prices, color='blue', marker='o')
plt.xlabel('Nearest School Distance')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Nearest School Distance')

# Scatter plot for Nearest Station Distance
plt.subplot(1, 3, 2)
plt.scatter(nearest_station_dist, prices, color='green', marker='s')
plt.xlabel('Nearest Station Distance')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Nearest Station Distance')

# Scatter plot for CBD Distance
plt.subplot(1, 3, 3)
plt.scatter(cbd_dist, prices, color='red', marker='o')
plt.xlabel('CBD Distance')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. CBD Distance')

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

# Extract relevant columns
land_area = all_data['LAND_AREA']
floor_area = all_data['FLOOR_AREA']
prices = all_data['PRICE']

# Create separate scatter plots for each feature
plt.figure(figsize=(12, 4))

# Scatter plot for LAND_AREA
plt.subplot(1, 2, 1)
plt.scatter(land_area, prices, color='blue', marker='o')
plt.xlabel('Land Area')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Land Area')

# Scatter plot for FLOOR_AREA
plt.subplot(1, 2, 2)
plt.scatter(floor_area, prices, color='green', marker='s')
plt.xlabel('Floor Area')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Floor Area')

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

# Extract relevant columns
bathrooms = all_data['BATHROOMS']
bedrooms = all_data['BEDROOMS']
garage = all_data['GARAGE']
prices = all_data['PRICE']

# Create separate scatter plots for each feature
plt.figure(figsize=(13, 6))

# Scatter plot for BATHROOMS
plt.subplot(1, 3, 1)
plt.scatter(bathrooms, prices, color='blue', marker='s')
plt.xlabel('Number of Bathrooms')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Number of Bathrooms')

# Scatter plot for BEDROOMS
plt.subplot(1, 3, 2)
plt.scatter(bedrooms, prices, color='green', marker='s')
plt.xlabel('Number of Bedrooms')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Number of Bedrooms')

# Scatter plot for GARAGE
plt.subplot(1, 3, 3)
plt.scatter(garage, prices, color='red', marker='s')
plt.xlabel('Garage')
plt.ylabel('Housing Price')
plt.title('Housing Price vs. Garage')

# Adjust layout
plt.tight_layout()

# Show the plots
plt.show()

data1=all_data.drop(columns='ADDRESS', axis =1)
data1=data1.drop(columns='SUBURB', axis =1)
data1=data1.drop(columns='NEAREST_STN', axis =1)
data1=data1.drop(columns='DATE_SOLD', axis =1)
data1=data1.drop(columns='NEAREST_SCH', axis =1)
data1

# Dividing dataset into label and feature sets
X = data1.drop(columns='PRICE', axis =1) #Features
Y = data1['PRICE'] #Labels
print(X)
print(Y)

# splitting
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)
print(X.shape, X_train.shape, X_test.shape)

# Linear Regression
# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the scaler to training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Apply the same scaling to test data
X_test_scaled = scaler.transform(X_test)

# Hyperparameter Tuning
linear_model = LinearRegression(fit_intercept=True)
linear_model.fit(X_train_scaled, Y_train)

# Make Predictions
linear_predictions = linear_model.predict(X_test_scaled)

# Evaluate Model
linear_rmse = mean_squared_error(Y_test, linear_predictions, squared=False)
linear_r2 = r2_score(Y_test, linear_predictions)

# Print Results
print("Linear Regression Results:")
print("Root Mean Squared Error:", linear_rmse)
print("R-squared:", linear_r2)

# Gradient Boosting Regression Model

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

grid_gb = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')
grid_gb.fit(X_train, Y_train)

# Make Predictions
gb_tuned_predictions = grid_gb.predict(X_test)

# Evaluate Model
gb_tuned_rmse = mean_squared_error(Y_test, gb_tuned_predictions, squared=False)
gb_tuned_r2 = r2_score(Y_test, gb_tuned_predictions)

# Print Results
print("Gradient Boosting Regression Results:")
print("Best Parameters:", grid_gb.best_params_)

print("Root Mean Squared Error:", gb_tuned_rmse)
print("R-squared:", gb_tuned_r2)

# Random Forest Regression Model
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

random_search_rf = RandomizedSearchCV(
    RandomForestRegressor(),
    param_distributions=param_dist,
    n_iter=10,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42
)
random_search_rf.fit(X_train, Y_train)

# Make Predictions
rf_predictions = random_search_rf.predict(X_test)

# Evaluate Model
rf_rmse = mean_squared_error(Y_test, rf_predictions, squared=False)
rf_r2 = r2_score(Y_test, rf_predictions)

# Print Results
print("Random Forest Regression Results:")
print("Best Parameters:", random_search_rf.best_params_)
print("Root Mean Squared Error:", rf_rmse)
print("R-squared:", rf_r2)

# Visualize Feature Importances
feature_importances = grid_gb.best_estimator_.feature_importances_
sorted_indices = feature_importances.argsort()

plt.figure(figsize=(8, 4))
plt.barh(range(X.shape[1]), feature_importances[sorted_indices], tick_label=X.columns[sorted_indices])
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Gradient Boosting Feature Importances')
plt.show()

# Feature Importance Analysis
feature_importances = grid_gb.best_estimator_.feature_importances_
feature_names = X.columns

# Create a DataFrame for feature importances
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importances table
print("Gradient Boosting Feature Importances:")
print(feature_importance_df)

# Visualize Predictions vs. Actual Prices
plt.figure(figsize=(8, 6))
plt.scatter(Y_test, gb_tuned_predictions)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Gradient Boosting Predictions vs. Actual Prices')
plt.show()